<!doctype html><html lang=zh-ch><head><title>斯坦福机器学习课程(一) // AirChen Blog</title><link rel="shortcut icon" href=pikachu.ico><meta charset=utf-8><meta name=generator content="Hugo 0.100.2"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="AirChen"><meta name=description content><link rel=stylesheet href=https://blog.airchen-space.top/css/main.min.4a7ec8660f9a44b08c4da97c5f2e31b1192df1d4d0322e65c0dbbc6ecb1b863f.css><meta name=twitter:card content="summary"><meta name=twitter:title content="斯坦福机器学习课程(一)"><meta name=twitter:description content="机器学习的两种定义： 来自Arthur Samuel 的表述: &ldquo;the field of study that gives computers the ability to learn without being explicitly programmed.&ldquo;这是一种古老的非正式的定义。 Tom Mitchell 提供的一种现代化的定义: &ldquo;A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.&rdquo; 举个例子，跳棋游戏
E = 玩跳棋的经验
T = 下跳棋的过程
P = 下场跳棋获胜的可能性
机器学习分为监督学习和非监督学习: 监督学习是指给出数据集合和单个数据子集对应的输出结果，认为这些数据集和输出结果之间存在某种关系。 监督学习可以分为回归和分类，回归对应输出的结果可以拟合连续性的，分类对应的输出结果对应是离散的。举个例子，给出一个男性或者女性的照片，预测照片上人物的年龄，因为输出结果是一系列连续的预测，所以这属于回归问题；如果通过这个照片来预测照片上人物的年龄是属于哪个学龄阶段(高中、大学、研究生)，由于结果都是一段一段分离的时间，所以认为这是一种分类问题。"><meta property="og:title" content="斯坦福机器学习课程(一)"><meta property="og:description" content="机器学习的两种定义： 来自Arthur Samuel 的表述: &ldquo;the field of study that gives computers the ability to learn without being explicitly programmed.&ldquo;这是一种古老的非正式的定义。 Tom Mitchell 提供的一种现代化的定义: &ldquo;A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.&rdquo; 举个例子，跳棋游戏
E = 玩跳棋的经验
T = 下跳棋的过程
P = 下场跳棋获胜的可能性
机器学习分为监督学习和非监督学习: 监督学习是指给出数据集合和单个数据子集对应的输出结果，认为这些数据集和输出结果之间存在某种关系。 监督学习可以分为回归和分类，回归对应输出的结果可以拟合连续性的，分类对应的输出结果对应是离散的。举个例子，给出一个男性或者女性的照片，预测照片上人物的年龄，因为输出结果是一系列连续的预测，所以这属于回归问题；如果通过这个照片来预测照片上人物的年龄是属于哪个学龄阶段(高中、大学、研究生)，由于结果都是一段一段分离的时间，所以认为这是一种分类问题。"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.airchen-space.top/posts/ml_part1/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-01-13T17:19:51+08:00"><meta property="article:modified_time" content="2018-01-13T17:19:51+08:00"></head><body><header class=app-header><a href=https://blog.airchen-space.top/><img class=app-header-avatar src=/log.jpeg alt=AirChen></a><h1>AirChen Blog</h1><nav class=app-header-menu><a class=app-header-menu-item href=/>Home</a>
-
<a class=app-header-menu-item href=/daily/>Tags</a>
-
<a class=app-header-menu-item href=/about/>About</a></nav><p>以最放松的心态对待一切艰难</p><div class=app-header-social><a href=https://github.com/AirChen target=_blank rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github"><title>My Github</title><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a><a href=renzhichen2012@163.com target=_blank rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-mail"><title>My Email</title><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></header><main class=app-container><article class=post><header class=post-header><h1 class=post-title>斯坦福机器学习课程(一)</h1><div class=post-meta><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>Jan 13, 2018</div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock"><title>clock</title><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>1 min read</div></div></header><div class=post-content><h3 id=机器学习的两种定义>机器学习的两种定义：</h3><ol><li>来自Arthur Samuel 的表述: &ldquo;the field of study that gives computers the ability to learn without being explicitly programmed.&ldquo;这是一种古老的非正式的定义。</li><li>Tom Mitchell 提供的一种现代化的定义: &ldquo;A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.&rdquo;</li></ol><p>举个例子，跳棋游戏</p><p>E = 玩跳棋的经验</p><p>T = 下跳棋的过程</p><p>P = 下场跳棋获胜的可能性</p><h3 id=机器学习分为监督学习和非监督学习>机器学习分为监督学习和非监督学习:</h3><p>监督学习是指给出数据集合和单个数据子集对应的输出结果，认为这些数据集和输出结果之间存在某种关系。
监督学习可以分为回归和分类，回归对应输出的结果可以拟合连续性的，分类对应的输出结果对应是离散的。举个例子，给出一个男性或者女性的照片，预测照片上人物的年龄，因为输出结果是一系列连续的预测，所以这属于回归问题；如果通过这个照片来预测照片上人物的年龄是属于哪个学龄阶段(高中、大学、研究生)，由于结果都是一段一段分离的时间，所以认为这是一种分类问题。</p><p><img src=images/Regression.png alt=回归>
<img src=images/Classification.png alt=分类></p><p>非监督学习，从另一方面出发，我们并不知道从给出的数据中，可以得到什么样的结果。学习结果最终会得到一个数据的分析结果，当然我们也不必数据中每个量参与决策的作用。
类聚算法，给出1000篇左右的博客文章，通过对文章中词汇的出现频率、句子长短、文章的篇幅等信息，将1000篇文章归为许多类。
<a href=https://en.wikipedia.org/wiki/Cocktail_party_effect>鸡尾酒会算法</a></p><p><img src=images/SL.png alt=监督学习>
<img src=images/USL.png alt=非监督学习></p><h3 id=单变量线性回归>单变量线性回归</h3><p>一种通过单一变量进行预测的监督式学习模型，命特性量为x，结果为y，我们的假设x和y之间存在下面等式中的关系：</p><p><img src=images/Hypothesis.png alt=假设函数></p><p>这个等式即假设函数，假设函数可以用来做预测，使用实际中的数据预测结果。但预测结果不一定准确，我们希望找到最合适的θ0和θ1，来表述这个等式，使假设函数的实际预测能力最好。通过代价函数，我们可以来判断假设函数的准确性，命代价函数如下：</p><p><img src=images/cost.png alt=代价函数></p><p>m即数据集大小，给定的已知数据集有多大，这个值就为多少。我们用代价函数来计算θ0和θ1的值，代价函数的值越小代表假设函数的模拟效果越好，预测能力约准确。
<code>验证</code>将假设函数简化一下，保留一个参数θ1，得到下面式子：</p><p><img src=images/simp.png alt=简化></p><p>通过给参数θ1赋予不同的值可以得到代价函数和假设函数之间的关系：</p><p><img src=images/costhy.png alt=代价函数和假设函数对应关系></p><p><code>当代价函数的值最小时，假设函数的模拟效果最好。</code>同时类比一下，将θ0也带入，可以知道，通过参数θ0和θ1，可以共同决定代价函数的取值，这时的任务就是找出代价函数的最小值。</p><p><img src=images/two.png alt=多参数></p><p>坡度下降法是求解代价函数最小值的一种有效方法，算法的总体思想是想在代价函数的映射关系图中，找出一条通往最小值点的最快路径：</p><p><img src=images/grand.png alt=坡度下降求解></p><p>图中出现两个极小值点，这说明梯度下降算法只能够求出局部极值点，不一定是全局的，取不同的初始值结果可能不同。但由于代价函数的图形总是出现为弧形结构，所以局部极小值往往都是全局的极小值。
其迭代式如下：</p><p><img src=images/grandient.png alt=坡度下降迭代公式></p><h3 id=多元线性回归>多元线性回归</h3><p>将特征量由原来的单一量x变为一个X向量，多元线性回归就是找出多个特征量和结果y之间的线性关系。
类比单变量线性回归，假设函数、代价函数和梯度下降算法的迭代变成了如下形式：</p><p><img src=images/multiReg.png alt=多元线性回归>
<img src=images/multiGD.png alt=多元线性回归坡度下降迭代公式></p><p>其中$$x_j^(i)$$特征量的右下角数字代表着一系列特征量中的某一个，右上角括号中的数字代表着第几个测试组。</p><h3 id=梯度下降算法的使用经验>梯度下降算法的使用经验</h3><p><img src=images/feature.png alt=特征量范围></p><p>如图，特征量在各自的取值范围相差不大的时候，梯度下降算法的下降速度更快，所以在考虑使用梯度下降来求解代价函数的极值时，可以先对特征量进行归一化，归一化的公式如下：</p><p><img src=images/mean.png alt=归一化公式></p><p>用特征值减去特征值得均值，之后再用其结果去除以特征值的取值范围(最大值减去最小值),这样可以大致将特征值得范围限制在-0.5到0.5的范围内。将每个特征值都做类似处理之后，使用梯度下降时，迭代次数会大大减少。</p><p>关于迭代系数<code>α</code>的取值,如果取值太大，会出现迭代的时候代价函数的值出现增长的情况，或者是开始是下降，下降一定次数之后出现增长的情况，这个时候应该将<code>α</code>的值取得小一些，但太小也不行，会造成迭代的此次过多，影响效率。
为此，可以从一个较小的值开始取，当代价函数的值小于我们设定的下降阈值，我们再降<code>α</code>值增加十倍，然后再迭代，出现增长再减小<code>α</code>值，直到代价函数的值不怎么变动时，停止迭代。</p><p><img src=images/alphavalue.png alt=α的取值></p><p>有时候，特征值的种类不是很多，为了达到更好的拟合效果，可以通过给特征量取次方的形式。</p><p><img src=images/features.png alt=特征值变化></p><h3 id=求解代价函数极值的另一种方法>求解代价函数极值的另一种方法</h3><p>类似与求解函数的极值，高数中用先求解方程的导数，再命其值为零的方法进行求解。这里也可以用类似的方法，将代价函数进行求偏导数处理，最后将得到的结果用矩阵的形式表述出来如下：</p><p><img src=images/normal.png alt=正态方程求极值></p><p>这种方法叫做正态方程求解，在特征值较少的时候，执行效率高于梯度下降算法，二者的比较如下：</p><p><img src=images/shome.png alt=比较></p><p>它不需要选择<code>α</code>值，不用进行迭代，但需要进行矩阵运算，算法的复杂度为$$o(n^3)$$。
学过线性代数的同学都会知道，$$X^T*X$$的值有可能不是可逆矩阵，这个时候，正态方程的方法就貌似不可用了，这里有两个办法可以解决这个问题：</p><ol><li>特征量之间可能有多余的，明显特征是两个特征量之间存在线性关系，如果发现这类特征量，删除其中任一一个量即可。</li><li>特征量太多，而实际的数据量样本太少，即m的取值可能是10，但n的取值为100+，这个时候，可以考虑去掉一些并不是那么重要的特征量。</li></ol><p>参考资料</p><p><a href=https://www.coursera.org/learn/machine-learning/home/welcome>Coursera课程</a></p><p><a href="%5Bhttps://www.youtube.com/watch?v=WnqQrPNYz5Q%5D(https://www.youtube.com/watch?v=WnqQrPNYz5Q)">梯度下降算法的原理讲解视频</a></p></div><div class=post-footer></div></article></main></body></html>